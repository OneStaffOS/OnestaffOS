# IT Help Desk Neural Network Chatbot

## 1. Problem Definition

### Scope & Limitations

**What the chatbot handles:**
- ğŸ” **Account & Access**: Password resets, login issues, MFA setup, account unlock
- ğŸŒ **Network Issues**: VPN, WiFi, connectivity, firewall problems
- ğŸ’» **Software Support**: Office 365, email, browser issues, software installation
- ğŸ–¥ï¸ **Hardware Problems**: Printers, monitors, keyboards, laptops
- ğŸ‘¥ **HR Systems**: Leave requests, payroll questions, benefits
- ğŸ›¡ï¸ **Security**: Phishing reports, virus alerts, security policies

**Limitations:**
- Cannot perform actual system changes (only provides instructions)
- Cannot access external systems or databases in real-time
- Limited to predefined intent categories
- Requires human escalation for complex/unique issues

### Expected Response Style
- **Concise**: Direct answers without unnecessary fluff
- **Step-by-step**: Numbered instructions for procedures
- **Technical**: Appropriate technical detail for IT support

---

## 2. Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    IT Help Desk Chatbot                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  User Query: "My VPN keeps disconnecting"                       â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚   Tokenizer     â”‚  Convert text â†’ token IDs                 â”‚
â”‚  â”‚   (BPE/WordPiece)â”‚                                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚           â”‚                                                     â”‚
â”‚           â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚   Embedding     â”‚  Token IDs â†’ Dense vectors (128-dim)      â”‚
â”‚  â”‚   Layer         â”‚                                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚           â”‚                                                     â”‚
â”‚           â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚   Transformer   â”‚  4 layers, 4 heads, 256 hidden           â”‚
â”‚  â”‚   Encoder       â”‚  ~2M parameters                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚           â”‚                                                     â”‚
â”‚           â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚   Intent        â”‚  Classify into 45 intent categories      â”‚
â”‚  â”‚   Classifier    â”‚  Softmax output                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚           â”‚                                                     â”‚
â”‚           â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚   Response      â”‚  Select response from intent              â”‚
â”‚  â”‚   Generator     â”‚  + Entity extraction                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚           â”‚                                                     â”‚
â”‚           â–¼                                                     â”‚
â”‚  Response: "VPN Troubleshooting Guide: 1. Verify..."           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Model Specifications

### Small Transformer Architecture

| Component | Specification |
|-----------|---------------|
| **Embedding Dimension** | 128 |
| **Hidden Dimension** | 256 |
| **Number of Layers** | 4 |
| **Attention Heads** | 4 |
| **Max Sequence Length** | 128 tokens |
| **Vocabulary Size** | ~5000 tokens |
| **Total Parameters** | ~2-3M |
| **Model Size** | ~10-15MB |

### Why This Architecture?

1. **Small & Fast**: Runs efficiently on CPU, ~50ms inference
2. **Sufficient for Classification**: Intent classification doesn't need GPT-4 scale
3. **Easy to Train**: Can train on a single GPU in < 1 hour
4. **Portable**: Can deploy on edge devices or serverless

---

## 4. Training Data

### Dataset Sources
- `knowledge-data.json`: 45 intents, ~650 patterns
- `Intent.json`: Greeting/conversation intents, ~200 patterns

### Data Split
- **Training**: 80% (~680 samples)
- **Validation**: 10% (~85 samples)
- **Test**: 10% (~85 samples)

---

## 5. File Structure

```
Backend/src/chatbot/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ knowledge-data.json          # Main IT support intents
â”œâ”€â”€ Intent.json                  # Greeting/conversation intents
â”œâ”€â”€ config.py                    # Model configuration
â”œâ”€â”€ preprocess.py                # Data preprocessing
â”œâ”€â”€ tokenizer.py                 # Custom BPE tokenizer
â”œâ”€â”€ model.py                     # Transformer model architecture
â”œâ”€â”€ train.py                     # Training script
â”œâ”€â”€ evaluate.py                  # Evaluation metrics
â”œâ”€â”€ inference.py                 # Inference/chat interface
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.json              # Processed training data
â”‚   â”œâ”€â”€ valid.json              # Validation data
â”‚   â””â”€â”€ test.json               # Test data
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ tokenizer.json          # Trained tokenizer
â”‚   â””â”€â”€ helpdesk_model.pt       # Trained model weights
â””â”€â”€ logs/
    â””â”€â”€ training.log            # Training logs
```

---

## 6. Quick Start

```bash
# 1. Install dependencies
pip install torch numpy scikit-learn tqdm

# 2. Preprocess data
python preprocess.py

# 3. Train model
python train.py

# 4. Evaluate
python evaluate.py

# 5. Run inference
python inference.py
```

---

## 7. Hardware Requirements

### Minimum (CPU Training)
- 8GB RAM
- 4-core CPU
- Training time: ~2-4 hours

### Recommended (GPU Training)
- 16GB RAM
- NVIDIA GPU with 4GB+ VRAM
- Training time: ~15-30 minutes

---

## 8. Metrics & Evaluation

| Metric | Target | Description |
|--------|--------|-------------|
| **Intent Accuracy** | >90% | Correct intent classification |
| **Top-3 Accuracy** | >98% | Correct intent in top 3 predictions |
| **Response Relevance** | >85% | Human-judged relevance score |
| **Confidence Calibration** | Low ECE | Predicted confidence matches accuracy |
| **Inference Latency** | <100ms | Time to generate response |
